{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по одному баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн: 24 нояб\n",
    "\n",
    "__Жесткий дедлайн: 27 нояб\n",
    "\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в телефонах, поисковой строке или приложении почты. Полученную систему вам нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex), чтобы ей можно было удобно пользоваться, а так же, чтобы убедиться, что все работает как надо. В этот раз вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей предположительно одинаковых по сложности. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы получилось в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а также отчет с подробным описанием техник, которые в применили для создания вашей системы. Не лишним будет также написать и о том, что у вас не получилось и почему.\n",
    "\n",
    "За часть с заданиями можно будет получить до __5__ баллов, за отчет – до __3__ баллов, 2 балл за доп вопросы, если возникнут, если вопросов не возникло, считаем, что 2 балла вы получили "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <26807948.1075842029936.JavaMail.evans@thyme>\n",
      "Date: Wed, 28 Nov 2001 13:30:11 -0800 (PST)\n",
      "From: john.zufferli@enron.com\n",
      "To: kori.loibl@enron.com\n",
      "Subject: Trade with John Lavorato\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Zufferli, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JZUFFER>\n",
      "X-To: Loibl, Kori </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Kloibl>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\ExMerge - Zufferli, John\\Sent Items\n",
      "X-Origin: ZUFFERLI-J\n",
      "X-FileName: john zufferli 6-26-02.PST\n",
      "\n",
      "This is a trade with OIL-SPEC-HEDGE-NG (John Lavorato's book) and John Zufferli's book CAND-PWR-PR\n",
      "\n",
      "CAND-PWR-PR buys from OIL-SPEC-HEDGE-NG Nymex Gas\n",
      "\n",
      "Cal 03\t\t\t38,500 MMBtu/day\t\t$3.2978\n",
      "Cal 04-Cal 05\t\t35,000 MMBtu/day\t\t$3.4482\n",
      "Cal 06-Cal 10\t\t40,000 MMBtu/day\t\t$3.7369\n",
      "Cal 11-Cal 15\t\t18,500 MMBtu/Day\t\t$4.2057\n",
      "\n",
      "Please Confirm Receipt\n",
      "\n",
      "\n",
      "John Z\n",
      "\n",
      "\n",
      "Message-ID: <25835861.1075842029959.JavaMail.evans@thyme>\n",
      "Date: Wed, 28 Nov 2001 12:47:48 -0800 (PST)\n",
      "From: john.zufferli@enron.com\n",
      "To: john.lavorato@enron.com\n",
      "Subject: Gas Hedges\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Zufferli, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JZUFFER>\n",
      "X-To: Lavorato, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jlavora>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\ExMerge - Zufferli, John\\Sent Items\n",
      "X-Origin: ZUFFERLI-J\n",
      "X-FileName: john zufferli 6-26-02.PST\n",
      "\n",
      "Some of my position is with the Alberta Term book, I will send you only the positions that I have directly with ENA. \n",
      "\n",
      "\n",
      "Message-ID: <28979867.1075842029988.JavaMail.evans@thyme>\n",
      "Date: Wed, 28 Nov 2001 07:20:00 -0800 (PST)\n",
      "From: john.zufferli@enron.com\n",
      "To: dawn.doucet@enron.com\n",
      "Subject: RE: CONFIDENTIAL\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Zufferli, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JZUFFER>\n",
      "X-To: Doucet, Dawn </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ddoucet>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\ExMerge - Zufferli, John\\Sent Items\n",
      "X-Origin: ZUFFERLI-J\n",
      "X-FileName: john zufferli 6-26-02.PST\n",
      "\n",
      "2\n",
      "\n",
      " -----Original Message-----\n",
      "From: \tDoucet, Dawn  \n",
      "Sent:\tWednesday, November 28, 2001 8:17 AM\n",
      "To:\tZufferli, John\n",
      "Subject:\tCONFIDENTIAL\n",
      "\n",
      "Morning John,\n",
      "I'm still working on the mini-PRC for Lavo.  Sean Lalani has not yet been ranked and rumour has it that he reports to you now.  Can you confirm and send me a number.  Thanks!\n",
      "\n",
      "\n",
      "Message-ID: <22052556.1075842030013.JavaMail.evans@thyme>\n",
      "Date: Tue, 27 Nov 2001 11:52:45 -0800 (PST)\n",
      "From: john.zufferli@enron.com\n",
      "To: jeanie.slone@enron.com\n",
      "Subject: Calgary Analyst/Associate\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Zufferli, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JZUFFER>\n",
      "X-To: Slone, Jeanie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jslone>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\ExMerge - Zufferli, John\\Sent Items\n",
      "X-Origin: ZUFFERLI-J\n",
      "X-FileName: john zufferli 6-26-02.PST\n",
      "\n",
      "Analyst\t\t\t\t\tRank\n",
      "\n",
      "Stephane Brodeur\t\t\t1\n",
      "Chad Clark\t\t\t\t1\n",
      "Ian Cooke\t\t\t\t3\n",
      "Lon Draper\t\t\t\t1\n",
      "Fabian Taylor\t\t\t\t2\n",
      "Carlos Torres\t\t\t\t3\n",
      "Ryan Watt\t\t\t\t1\n",
      "\n",
      "Associate\n",
      "\n",
      "Cooper Richey\t\t\t\t1\n",
      "\n",
      "\n",
      "\n",
      "Message-ID: <28618979.1075842030037.JavaMail.evans@thyme>\n",
      "Date: Mon, 26 Nov 2001 10:48:43 -0800 (PST)\n",
      "From: john.zufferli@enron.com\n",
      "To: livia_zufferli@monitor.com\n",
      "Subject: RE: ali's essays\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Zufferli, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JZUFFER>\n",
      "X-To: 'Livia_Zufferli@Monitor.com@ENRON'\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\ExMerge - Zufferli, John\\Sent Items\n",
      "X-Origin: ZUFFERLI-J\n",
      "X-FileName: john zufferli 6-26-02.PST\n",
      "\n",
      "i think the YMCA has a class that is for people recovering from heart-attacks\n",
      "i remeber something about that\n",
      "\n",
      " -----Original Message-----\n",
      "From: \tLivia_Zufferli@Monitor.com@ENRON  \n",
      "Sent:\tMonday, November 26, 2001 11:44 AM\n",
      "To:\tZufferli, John\n",
      "Subject:\tRE: ali's essays\n",
      "\n",
      "\n",
      "i don't know about the heart classes.  i'll look into it, but her dr (ravi)\n",
      "isn't offering up any suggestions or anything.  she saw him before the\n",
      "surgery in august, and he said things were okay.  i really don't think he's\n",
      "too helpful.\n",
      "\n",
      "she is lazy -- but it really frustrates me that she doesn't want to help\n",
      "herself.  i told her that not walking is like not taking her heart\n",
      "medication.  that didn't seem to resonate.  dad is going to go to the YMCA\n",
      "tomorrow and maybe get a membership for both of them -- they have a walking\n",
      "track there (at least it's something to do in the winter).  when she was\n",
      "down this weekend, we walked around the craft show (at the Exhibition\n",
      "place) and she said that was a lot of exercise (2 hrs).  The only problem\n",
      "is that we were just strolling, and not really walking very fast.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    John.Zufferli@\n",
      "                    enron.com            To:     Livia_Zufferli@Monitor.com\n",
      "                                         cc:\n",
      "                    11/26/2001           Subject:     RE: ali's essays\n",
      "                    01:41 PM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "just send the essay at home\n",
      "\n",
      "I don't know what to do about mom, i don't think fear is the only thing\n",
      "holding her back , i think she is lazy\n",
      "\n",
      "is there a heart health class in Sudbury that has excercise regimines as\n",
      "well as diets?\n",
      "\n",
      "when is the last time she saw her doctor\n",
      "\n",
      "    -----Original Message-----\n",
      "   From:   Livia_Zufferli@Monitor.com@ENRON\n",
      "   Sent:   Monday, November 26, 2001 11:19 AM\n",
      "   To:     Zufferli, John\n",
      "   Subject:  ali's essays\n",
      "\n",
      "   Hi John\n",
      "\n",
      "   How was Thanksgiving?  Was the baby shower fun?\n",
      "\n",
      "   I was wondering if you'd have some time to read over Ali's Chicago\n",
      "   essays\n",
      "   later tonight?  He's going to submit them on Wednesday.  Let me know if\n",
      "   that's okay.  Do you have a printer at home?  Can I send them to your\n",
      "   home\n",
      "   account?  (I don't think Ali will be done before about 8pm or so\n",
      "   tonight).\n",
      "\n",
      "   PS:  We need to talk about mom.  I saw her this weekend -- she's gained\n",
      "   a\n",
      "   lot of weight, and hasn't been exercising at all.  Dad's pretty\n",
      "   frustrated\n",
      "   because all she does is watch tv.  I had a talk with her yesterday\n",
      "   telling\n",
      "   her that she has more risk of having a heart attack if she doesn't walk\n",
      "   /\n",
      "   exercise than if she exerts herself when exercising (i think she's\n",
      "   afraid\n",
      "   of having a heart attack while exercising).  We need to do something --\n",
      "   she's 170lbs now, and should be at around 140lbs to be healthy.\n",
      "\n",
      "   Livia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "This e-mail is the property of Enron Corp. and/or its relevant affiliate\n",
      "and may contain confidential and privileged material for the sole use of\n",
      "the intended recipient (s). Any review, use, distribution or disclosure by\n",
      "others is strictly prohibited. If you are not the intended recipient (or\n",
      "authorized to receive for the recipient), please contact the sender or\n",
      "reply to Enron Corp. at enron.messaging.administration@enron.com and delete\n",
      "all copies of the message. This e-mail (and any attachments hereto) are not\n",
      "intended to be an offer (or an acceptance) and do not create or evidence a\n",
      "binding and enforceable contract between Enron Corp. (or any of its\n",
      "affiliates) and the intended recipient or any other party, and may not be\n",
      "relied on by anyone as the basis of a contract by estoppel or otherwise.\n",
      "Thank you.\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_emails = emails.tail()\n",
    "test_emails = test_emails.copy()\n",
    "\n",
    "for test_email in test_emails['message'].values.tolist():\n",
    "    print(test_email)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (1 балл).__ Очистите корпус текстов по вашему усмотрению. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст. Оценка будет выставляться по близости вашего результата к этому идеалу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import message_from_string\n",
    "import re\n",
    "\n",
    "def get_message_body(message: str) -> str:\n",
    "    # Парсим письмо\n",
    "    email_message = message_from_string(message)\n",
    "    # Получаем тело письма\n",
    "    return email_message.get_payload(decode=True).decode(email_message.get_content_charset() or 'utf-8')\n",
    "\n",
    "def clear_message_body(message: str) -> str:\n",
    "    # Получаем тело письма\n",
    "    body = get_message_body(message)\n",
    "\n",
    "    # Удаляем строки с ключами To:, From:, RE:, и подобными\n",
    "    meta_info_pattern = r'(To:|From:|Subject:|CC:|BCC:|RE:|Sent:|cc:|RE:).*$'\n",
    "    body = re.sub(meta_info_pattern, '', body, flags=re.MULTILINE)\n",
    "\n",
    "    # Удаляем текст, идущий после -----Original Message----- до первой пустой строки\n",
    "    original_message_pattern = r'-----Original Message-----.*?\\n\\n'\n",
    "    body = re.sub(original_message_pattern, '', body, flags=re.DOTALL)\n",
    "\n",
    "    # Удаляем адреса электронной почты\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
    "    body = re.sub(email_pattern, '', body)\n",
    "\n",
    "    # Удаляем даты (формат: день, месяц, год, например: 14 May 2001 или 28/11/2001)\n",
    "    date_pattern = r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{1,2}\\s+[A-Za-z]{3,9}\\s+\\d{2,4})\\b'\n",
    "    body = re.sub(date_pattern, '', body)\n",
    "\n",
    "    # Удаляем время (формат: 14:30, 8:17 AM, 16:39:00)\n",
    "    time_pattern = r'\\b(?:[01]?\\d|2[0-3]):([0-5]?\\d)(?::([0-5]?\\d))?\\b|\\d{1,2}:\\d{2}\\s?(AM|PM)?\\b'\n",
    "    body = re.sub(time_pattern, '', body)\n",
    "\n",
    "    # Удаляем disclaimers (юридические уведомления, например текст после *****)\n",
    "    disclaimer_pattern = r'\\n\\*+.*?This e-mail.*?(?:\\n\\*+|\\Z)'\n",
    "    body = re.sub(disclaimer_pattern, '', body, flags=re.DOTALL)\n",
    "\n",
    "    # Убираем дополнительные пробелы\n",
    "    body = re.sub(r'\\s+', ' ', body).strip()\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>This is a trade with OIL-SPEC-HEDGE-NG (John L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>Some of my position is with the Alberta Term b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517398</th>\n",
       "      <td>2 Morning John, I'm still working on the mini-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517400</th>\n",
       "      <td>i think the YMCA has a class that is for peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  message\n",
       "517396  This is a trade with OIL-SPEC-HEDGE-NG (John L...\n",
       "517397  Some of my position is with the Alberta Term b...\n",
       "517398  2 Morning John, I'm still working on the mini-...\n",
       "517399  Analyst Rank Stephane Brodeur 1 Chad Clark 1 I...\n",
       "517400  i think the YMCA has a class that is for peopl..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Протестируем на 5 элементах датасета\n",
    "test_emails['message'] = test_emails['message'].apply(clear_message_body)\n",
    "# Дропнем ненужный столбец\n",
    "test_emails = test_emails.drop('file', axis = 1)\n",
    "\n",
    "test_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a trade with OIL-SPEC-HEDGE-NG (John Lavorato's book) and John Zufferli's book CAND-PWR-PR CAND-PWR-PR buys from OIL-SPEC-HEDGE-NG Nymex Gas Cal 03 38,500 MMBtu/day $3.2978 Cal 04-Cal 05 35,000 MMBtu/day $3.4482 Cal 06-Cal 10 40,000 MMBtu/day $3.7369 Cal 11-Cal 15 18,500 MMBtu/Day $4.2057 Please Confirm Receipt John Z\n",
      "Some of my position is with the Alberta Term book, I will send you only the positions that I have directly with ENA.\n",
      "2 Morning John, I'm still working on the mini-PRC for Lavo. Sean Lalani has not yet been ranked and rumour has it that he reports to you now. Can you confirm and send me a number. Thanks!\n",
      "Analyst Rank Stephane Brodeur 1 Chad Clark 1 Ian Cooke 3 Lon Draper 1 Fabian Taylor 2 Carlos Torres 3 Ryan Watt 1 Associate Cooper Richey 1\n",
      "i think the YMCA has a class that is for people recovering from heart-attacks i remeber something about that i don't know about the heart classes. i'll look into it, but her dr (ravi) isn't offering up any suggestions or anything. she saw him before the surgery in august, and he said things were okay. i really don't think he's too helpful. she is lazy -- but it really frustrates me that she doesn't want to help herself. i told her that not walking is like not taking her heart medication. that didn't seem to resonate. dad is going to go to the YMCA tomorrow and maybe get a membership for both of them -- they have a walking track there (at least it's something to do in the winter). when she was down this weekend, we walked around the craft show (at the Exhibition place) and she said that was a lot of exercise (2 hrs). The only problem is that we were just strolling, and not really walking very fast. John.Zufferli@ enron.com PM just send the essay at home I don't know what to do about mom, i don't think fear is the only thing holding her back , i think she is lazy is there a heart health class in Sudbury that has excercise regimines as well as diets? when is the last time she saw her doctor Hi John How was Thanksgiving? Was the baby shower fun? I was wondering if you'd have some time to read over Ali's Chicago essays later tonight? He's going to submit them on Wednesday. Let me know if that's okay. Do you have a printer at home? Can I send them to your home account? (I don't think Ali will be done before about 8pm or so tonight). PS: We need to talk about mom. I saw her this weekend -- she's gained a lot of weight, and hasn't been exercising at all. Dad's pretty frustrated because all she does is watch tv. I had a talk with her yesterday telling her that she has more risk of having a heart attack if she doesn't walk / exercise than if she exerts herself when exercising (i think she's afraid of having a heart attack while exercising). We need to do something -- she's 170lbs now, and should be at around 140lbs to be healthy. Livia\n"
     ]
    }
   ],
   "source": [
    "for msg in test_emails['message'].values:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['message'] = emails['message'].apply(clear_message_body)\n",
    "# Дропнем ненужный столбец\n",
    "emails = emails.drop('file', axis=1)\n",
    "# сохраним полученный датасет\n",
    "emails.to_csv('cleared_emails.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Edison International CFO Disputes \\'Technical Insolvency\\' Claim By Mark Golden Dow Jones Energy Service (Copyright (c) 2000, Dow Jones & Company, Inc.) NEW YORK -(Dow Jones)- Edison International\\'s treasurer and chief financial officer, Ted Craver, disputed Tuesday a claim made last week that California\\'s utilities risk \"technical insolvency\" in 2001. Since May, California\\'s three investor-owned utilities have been spending far more to purchase needed electricity than they are getting paid by customers. As a result, the undercollections in the \"transition revenue account\" of Edison\\'s regulated utility unit, Southern California Edison, grew by $1.97 billion between May 1 and August 31, the company reported to the Securities Exchange Commission. An article Wednesday in the Wall Street Journal, which is published by Dow Jones & Co., said that \"if the cost of wholesale power continues to exceed the price these utilities are allowed to bill their customers, as currently seems likely, they could become technically insolvent sometime next year.\" \"The way this term was used insinuated that you take the transition revenue account, and whatever the gross number is equates to the amount of debt that you would have to raise,\" Craver said in an interview. \"That is not at all the way it works. That is not an accurate portrayal.\" California Gov. Gray Davis acknowledged the concerns Friday, saying, \"California consumers have a legitimate need for California\\'s utilities to remain solvent, and the state must be committed toward that end.\" But Edison\\'s Craver said the situation is not that simple. \"That transition revenue account balance says nothing about what we have to fund in the market,\" Craver said. \"Southern California Edison has some 37 balancing and memo accounts set up by the California Public Utilities Commission. What we have to fund is the net amount of all of them, plus whatever else is going on, and the net figure is considerably smaller than the $1.97 billion. It\\'s like picking out one line item on the balance sheet and saying this is all that matters. No accountant does that or looks at it that way.\" Craver also said that the term \"technical insolvency\" has no specific meaning to him. A PG&E Corp (PCG) spokesman declined to comment when asked this week if it risks technical insolvency next year. The company is seeking refunds from independent generators that have been selling power to the state\\'s utilities, and for the future it has asked the Federal Energy Regulatory Commission to consider a return to cost-based pricing specific to each generating station in the state. Utility bond analysts, nevertheless, are concerned about California utility debt. \"I don\\'t believe the utilities are going to go bankrupt, but they don\\'t have any assurance from the legislature, governor or regulators,\" Bear Stearns\\'s Ted Olshanski said. \"The uncertainty is critical.\" Bear Stearns has a negative outlook on Edison and PG&E bonds due to the uncertainty on the recovery of these energy costs. This summer\\'s period of undercollections has continued into the fall, with the California\\'s wholesale market for Wednesday power at $135 a megawatt-hour, compared with the $65/MWh that SCE gets from customers. The current best asking price for on-peak power fall all of 2001 in southern California is $85/MWh. When asked how long Edison can continue to fund the difference between its purchase power costs and the revenue it receives from customers, Craver responded: \"How long can ratepayers or anyone afford those prices, which don\\'t have anything to do with reality? The market mechanisms don\\'t work.\" California\\'s utilities are hoping to make significant changes to the market, and some of those changes would require action by the state legislature, which is out of session until December. \"I don\\'t know that this is something that waits for the legislature,\" Craver said. \"There are a lot of things that can be done before the legislature meets. We have significant issues before the FERC and the CPUC.\" Olshanski said he hoped for a political solution. \"We\\'re not sure how its going to end,\" he said. \"We expect regulators and legislators to step up and provide for a constructive resolution. In the past, the California legislature has approved measures to make companies whole.\" -By Mark Golden, Dow Jones Newswires; 201-938-4604;'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.sample()['message'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('cleared_emails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы считаете, что результат получается лучше без нее, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "Описанная система будет состоять из двух частей: дополнение слова до целого и генерация продолжения текста (или вариантов продолжений). Начнем с первой части.\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова за $O(n + m)$, где $m$ – число подходящих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (1 балл).__ Допишите префиксное дерево для поиска слов по префиксу. Ваше дерево должно работать за $O(n + m)$ операции, в противном случае вы не получите баллов за это задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        # словарь с буквами, которые могут идти после данной вершины\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            current_node = self.root\n",
    "            for char in word:\n",
    "                if char not in current_node.children:\n",
    "                    current_node.children[char] = PrefixTreeNode()\n",
    "                current_node = current_node.children[char]\n",
    "            current_node.is_end_of_word = True\n",
    "\n",
    "    def search_prefix(self, prefix: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        \n",
    "        for char in prefix:\n",
    "            if char in current_node.children:\n",
    "                current_node = current_node.children[char]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        def get_bottom_prefixes(node: PrefixTreeNode, path: str) -> List[str]:\n",
    "            results = []\n",
    "            if node.is_end_of_word:\n",
    "                results.append(path)\n",
    "            for char, child in node.children.items():\n",
    "                results.extend(get_bottom_prefixes(child, path + char))\n",
    "            return results\n",
    "\n",
    "        return get_bottom_prefixes(current_node, prefix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его встречаемости в корпусе.\n",
    "\n",
    "__Задание 3 (1 балл).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCompletor:\n",
    "    def __init__(self, corpus: List[str]):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        self.word_count_dict = {}\n",
    "        \n",
    "        self.words_count = 0\n",
    "        \n",
    "        for corpus_item in corpus:\n",
    "            self.words_count += len(corpus_item)\n",
    "            for word in corpus_item:\n",
    "                if word in self.word_count_dict.keys():\n",
    "                    self.word_count_dict[word] += 1\n",
    "                else:\n",
    "                    self.word_count_dict[word] = 1\n",
    "\n",
    "        self.word_count_dict = {key: value / self.words_count for key, value in self.word_count_dict.items()}\n",
    "        \n",
    "        self.prefix_tree = PrefixTree(self.word_count_dict.keys())\n",
    "        \n",
    "\n",
    "    def get_words_and_probs(self, prefix: str) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words = self.prefix_tree.search_prefix(prefix)\n",
    "        \n",
    "        probs = [self.word_count_dict[word] for word in words]\n",
    "        \n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему несколько следующих слов с учетом дописанного. Для этого мы воспользуемся n-граммами и будем советовать n следующих слов. Но сперва нужно получить n-граммную модель.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "Тогда, нам нужно оценить $P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (1 балл).__ Напишите класс для n-граммной модели. Понятное дело, никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n):\n",
    "        # максимальная длинна N-граммы\n",
    "        self.n = n \n",
    "        # Счётчик частот N-грамм\n",
    "        self.ngram_counts = Counter()\n",
    "        # Счётчик частот контекстов (первых n-1 слов)\n",
    "        self.context_counts = Counter()\n",
    "        \n",
    "        # Построение N-грамм из корпуса\n",
    "        for sentence in corpus:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            for word_index in range(sentence_length):\n",
    "                for ngram_length in range(1, sentence_length - word_index + 1):  \n",
    "                    ngram = tuple(sentence[word_index : word_index + ngram_length])\n",
    "                    self.ngram_counts[ngram] += 1\n",
    "                    # Контексты существуют только для n-грамм длиной > 1\n",
    "                    if len(ngram) > 1:  \n",
    "                        context = ngram[:-1]\n",
    "                        self.context_counts[context] += 1\n",
    "                \n",
    "    def get_next_words_and_probs(self, prefix: list) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после prefix,\n",
    "        а так же список вероятностей этих слов\n",
    "        \"\"\"\n",
    "        \n",
    "        # Возможные следующие слова\n",
    "        next_words = []  \n",
    "        # Вероятности этих слов\n",
    "        probs = []  \n",
    "        # Преобразуем prefix в кортеж для сопоставления с n-граммами\n",
    "        context = tuple(prefix)  \n",
    "        # Найти все n-граммы, начинающиеся с данного контекста\n",
    "        for ngram, count in self.ngram_counts.items():\n",
    "\n",
    "            if ngram[:-1] == context:  # Проверка, соответствует ли n-грамма данному контексту\n",
    "                next_word = ngram[-1]\n",
    "                next_words.append(next_word)\n",
    "                \n",
    "                # Вычисляем вероятность слова как P(next_word | context)\n",
    "                context_count = self.context_counts[context]\n",
    "                probs.append(count / context_count)\n",
    "\n",
    "        return next_words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (1 балл).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. Если вы справитесь, то сможете можете добавить опцию поддержки нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor, n_gram_model):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(self, text: Union[str, list], n_words=3, n_texts=1) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        suggestion = []\n",
    "        \n",
    "        if(isinstance(text, str)):\n",
    "            words = text.strip().split()\n",
    "        elif(isinstance(text, list)):\n",
    "            words = text\n",
    "        else:\n",
    "            raise ValueError('Недопустимый тип text')\n",
    "        \n",
    "        if(len(words) == 0):\n",
    "            return []\n",
    "        # получим последнее слово\n",
    "        last_word = words[-1]\n",
    "        # получаем варианты дополнения последнего слова до целого\n",
    "        completor_words, completor_probs = self.word_completor.get_words_and_probs(last_word)\n",
    "        # если completor_words вернул дополненные слова, то берем слово с наибольшей вероятностью\n",
    "        last_word = completor_words[completor_probs.index(max(completor_probs))] if len(completor_words) > 0 else last_word\n",
    "        # заменяем последнее слово\n",
    "        words[-1] = last_word\n",
    "        \n",
    "        suggestion.append(last_word)\n",
    "        \n",
    "        n = self.n_gram_model.n\n",
    "        # получаем контекст (n - 1) слов\n",
    "        context = words[-(n - 1):]  if n > 1 else []\n",
    "        \n",
    "        for _ in range(n_words):\n",
    "            # получаем предсказания следующих n_word слов\n",
    "            n_gram_words, n_gram_probs = self.n_gram_model.get_next_words_and_probs(context)\n",
    "            \n",
    "            if len(n_gram_words) == 0:\n",
    "                break\n",
    "            # берем слово с макс вероятностью\n",
    "            next_word = n_gram_words[n_gram_probs.index(max(n_gram_probs))]\n",
    "            \n",
    "            suggestion.append(next_word)\n",
    "            # добавляем в контекст\n",
    "            context = context[1:] + [next_word] if n > 1 else [next_word]\n",
    "            \n",
    "        suggestions.append(suggestion)\n",
    "            \n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время довести вашу систему до ума. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным, __кроме использования дополнительных данных__. Главное – вы должны обернуть вашу систему в пользовательский интерфейс с помощью [reflex](https://github.com/reflex-dev/reflex). В нем можно реализовать почти любой функционал по вашему желанию.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать.\n",
    "\n",
    "При сдаче решения прикрепите весь ваш __код__, __отчет__ по второй части и __видео__ с демонстрацией работы вашей системы. Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
